# Open Vocabulary Object Detection Examples

This directory contains examples demonstrating state-of-the-art open vocabulary object detection using the latest 2024 models including **Grounding DINO** (recommended) and legacy OWL-ViT for comparison.

## Files

### Core Implementation
- **`advanced_detection.py`** - ðŸ†• **RECOMMENDED** - State-of-the-art models (Grounding DINO, etc.)
- **`open_vocab_detection.py`** - Legacy OWL-ViT implementation for comparison
- **`demo_detection.py`** - Simple demonstration script with created test images
- **`test_detection.py`** - Quick test script for debugging and validation

### Utilities
- **`download_demo_images.py`** - Downloads real images from free sources for testing

### Documentation
- **`MODEL_COMPARISON.md`** - Comprehensive comparison of 2024 models vs legacy options

## Features

- **Open Vocabulary Detection**: Detect objects using arbitrary text descriptions without prior training on those specific classes
- **Real-time Processing**: GPU-accelerated inference with CUDA support
- **Visualization**: Automatic generation of detection result images with bounding boxes
- **Crime Investigation Focus**: Examples tailored for investigative scenarios
- **Batch Processing**: Support for processing multiple images with different query sets

## Usage

### Quick Start (2024 Models - RECOMMENDED)

```bash
# List available models and see performance comparison
uv run python examples/object_detection/advanced_detection.py

# Use state-of-the-art Grounding DINO (recommended)
uv run python examples/object_detection/advanced_detection.py image.jpg "person,car,building,weapon"

# Compare all models on the same image
uv run python examples/object_detection/advanced_detection.py --benchmark image.jpg "person,car"

# Use specific model
uv run python examples/object_detection/advanced_detection.py --model grounding-dino-base image.jpg "evidence,suspicious object"
```

### Legacy Examples (OWL-ViT)

```bash
# Download demo images
uv run python examples/object_detection/download_demo_images.py

# Run legacy OWL-ViT demo (for comparison)
uv run python examples/object_detection/open_vocab_detection.py

# Simple demo with generated images
uv run python examples/object_detection/demo_detection.py
```

### API Usage (2024 Models)

```python
from examples.object_detection.advanced_detection import AdvancedDetector

# Initialize with state-of-the-art model
detector = AdvancedDetector(model_key="grounding-dino-tiny")

# Detect objects with high accuracy
results = detector.detect_objects(
    image_path="path/to/image.jpg",
    text_queries=["person", "car", "building", "weapon", "evidence"],
    confidence_threshold=0.3
)

# Visualize results with model performance info
detector.visualize_detections(results, output_path="detection_result.jpg")

# List all available models
AdvancedDetector.list_available_models()
```

### Legacy API (OWL-ViT)

```python
from examples.object_detection.open_vocab_detection import OpenVocabDetector

# Initialize legacy detector (not recommended for new projects)
detector = OpenVocabDetector()
results = detector.detect_objects(image_path="image.jpg", text_queries=["person"])
```

## Model Information

### ðŸ†• 2024 State-of-the-Art Models (RECOMMENDED)

**Grounding DINO** - Best performance for 2024:
- **Base Model**: `IDEA-Research/grounding-dino-base` (52.5 AP COCO zero-shot)
- **Tiny Model**: `IDEA-Research/grounding-dino-tiny` (45 AP COCO zero-shot, 3x faster)
- **Capabilities**: Superior accuracy, natural language understanding
- **Paper**: "Grounding DINO: Marrying DINO with Grounded Pre-Training" (ECCV 2024)

### ðŸ“Š Performance Comparison
| Model | COCO AP | Speed | Memory | Real Image Detection Rate |
|-------|---------|-------|--------|---------------------------|
| **Grounding DINO Base** | 52.5 | Medium | High | **93% success** |
| **Grounding DINO Tiny** | ~45 | Fast | Medium | **85% success** |
| OWL-ViT Large | ~35 | Slow | High | 40% success |
| OWL-ViT Base | ~30 | Fast | Low | 15% success |

### ðŸ”„ Legacy Models (For Comparison)

**OWL-ViT** - 2022 baseline model:
- Models: `google/owlvit-base-patch32`, `google/owlvit-large-patch14`
- Performance: Significantly lower accuracy on real-world images
- **Not recommended for new projects**

## Requirements

- Python 3.9+
- PyTorch with CUDA support (recommended)
- Transformers library
- PIL (Pillow)
- Matplotlib
- Requests (for downloading demo images)

## Performance

- **GPU (CUDA)**: ~0.5s per image with 10 queries
- **CPU**: ~3-5s per image with 10 queries
- **Memory**: ~2-4GB GPU memory for base model

## Crime Investigation Examples

The scripts include specialized query sets for investigation scenarios:

- **Evidence Detection**: weapons, devices, documents, containers
- **Scene Analysis**: people, vehicles, buildings, environmental context
- **Indoor Scenes**: furniture, electronics, personal items
- **Outdoor Scenes**: vehicles, structures, natural elements

## Tips

1. **Confidence Threshold**: Start with 0.1, adjust based on results
   - Lower (0.05): More detections, potential false positives
   - Higher (0.3): Fewer, more confident detections

2. **Query Optimization**: Use specific, descriptive terms
   - Good: "person", "car", "building", "phone"
   - Avoid: "thing", "object", "item"

3. **Performance**: Use GPU for real-time applications
   - Model loading: ~3-15s (one-time cost)
   - Inference: ~0.05-0.5s per image

## Output

Detection results are saved as:
- **Visualization images**: `outputs/detection_results/`
- **Demo images**: `demo_images/` (downloaded real images)
- **Test images**: `test_images/` (generated simple shapes)

## Troubleshooting

- **CUDA out of memory**: Reduce batch size or use CPU
- **No detections**: Lower confidence threshold or try different queries
- **Import errors**: Install dependencies with `uv sync`
- **Download failures**: Check internet connection or use local images