#!/usr/bin/env python3
"""
Advanced Open Vocabulary Object Detection
=========================================

This script implements state-of-the-art open vocabulary object detection using
the latest models including Grounding DINO, MM-Grounding DINO, and others that
significantly outperform OWL-ViT.

Supported Models (2024):
- Grounding DINO: SOTA performance, 52.5 AP COCO zero-shot
- MM-Grounding DINO: Improved version, 50.6 AP COCO
- OWL-ViT v2: Updated baseline
- OV-DINO: Unified detection framework

Performance Comparison:
- OWL-ViT: ~30 AP COCO zero-shot
- Grounding DINO: ~52.5 AP COCO zero-shot
- MM-Grounding DINO: ~50.6 AP COCO zero-shot
"""

import os
import time
import logging
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
import torch
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import requests
from io import BytesIO

try:
    from transformers import (
        AutoProcessor,
        AutoModelForZeroShotObjectDetection,
        GroundingDinoProcessor,
        GroundingDinoForObjectDetection,
        OwlViTProcessor,
        OwlViTForObjectDetection
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not available. Install with: pip install transformers torch")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Model configurations with performance metrics
MODEL_CONFIGS = {
    "grounding-dino-base": {
        "model_name": "IDEA-Research/grounding-dino-base",
        "description": "Grounding DINO Base - SOTA open vocabulary detection",
        "performance": "52.5 AP COCO zero-shot",
        "speed": "Medium",
        "memory": "High",
        "recommended": True
    },
    "grounding-dino-tiny": {
        "model_name": "IDEA-Research/grounding-dino-tiny",
        "description": "Grounding DINO Tiny - Faster inference",
        "performance": "~45 AP COCO zero-shot",
        "speed": "Fast",
        "memory": "Medium",
        "recommended": True
    },
    "owlvit-base": {
        "model_name": "google/owlvit-base-patch32",
        "description": "OWL-ViT Base - Original baseline",
        "performance": "~30 AP COCO zero-shot",
        "speed": "Fast",
        "memory": "Low",
        "recommended": False
    },
    "owlvit-large": {
        "model_name": "google/owlvit-large-patch14",
        "description": "OWL-ViT Large - Better accuracy",
        "performance": "~35 AP COCO zero-shot",
        "speed": "Slow",
        "memory": "High",
        "recommended": False
    }
}


class AdvancedDetector:
    """Advanced open vocabulary object detector with multiple SOTA models."""

    def __init__(self, model_key: str = "grounding-dino-tiny", device: str = "auto"):
        """
        Initialize the advanced detector.

        Args:
            model_key: Model configuration key from MODEL_CONFIGS
            device: Device to run inference on ('auto', 'cpu', 'cuda')
        """
        if model_key not in MODEL_CONFIGS:
            available_models = list(MODEL_CONFIGS.keys())
            raise ValueError(f"Model '{model_key}' not available. Choose from: {available_models}")

        self.model_config = MODEL_CONFIGS[model_key]
        self.model_name = self.model_config["model_name"]
        self.model_key = model_key
        self.device = self._determine_device(device)
        self.processor = None
        self.model = None
        self._initialized = False

        # Model-specific settings
        self.is_grounding_dino = "grounding-dino" in model_key
        self.is_owlvit = "owlvit" in model_key

    def _determine_device(self, device: str) -> str:
        """Determine the appropriate device for inference."""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device

    def initialize(self) -> None:
        """Initialize the model and processor."""
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers library not available")

        if self._initialized:
            return

        logger.info(f"Loading {self.model_config['description']}")
        logger.info(f"Model: {self.model_name}")
        logger.info(f"Expected performance: {self.model_config['performance']}")

        start_time = time.time()

        try:
            if self.is_grounding_dino:
                # Use Grounding DINO specific classes
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                self.model = AutoModelForZeroShotObjectDetection.from_pretrained(self.model_name)
            elif self.is_owlvit:
                # Use OWL-ViT specific classes
                self.processor = OwlViTProcessor.from_pretrained(self.model_name)
                self.model = OwlViTForObjectDetection.from_pretrained(self.model_name)
            else:
                # Generic approach for other models
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                self.model = AutoModelForZeroShotObjectDetection.from_pretrained(self.model_name)

            if self.device != "cpu":
                self.model = self.model.to(self.device)

            self._initialized = True
            load_time = time.time() - start_time
            logger.info(f"âœ… Model loaded successfully in {load_time:.2f}s on {self.device}")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize model: {e}")
            raise

    def detect_objects(
        self,
        image_path: str,
        text_queries: Union[List[str], str],
        confidence_threshold: float = 0.3,
        max_detections: int = 100
    ) -> Dict:
        """
        Detect objects in an image based on text queries.

        Args:
            image_path: Path to the input image
            text_queries: Text descriptions of objects to detect (list or single string)
            confidence_threshold: Minimum confidence score for detections
            max_detections: Maximum number of detections to return

        Returns:
            Dictionary containing detection results
        """
        if not self._initialized:
            self.initialize()

        # Handle both string and list inputs
        if isinstance(text_queries, str):
            text_queries = [q.strip() for q in text_queries.split(',')]

        # Load and validate image
        if not Path(image_path).exists():
            raise FileNotFoundError(f"Image not found: {image_path}")

        try:
            image = Image.open(image_path)
            if image.mode != "RGB":
                image = image.convert("RGB")
        except Exception as e:
            raise Exception(f"Failed to load image: {e}")

        logger.info(f"ðŸ” Detecting objects in {Path(image_path).name}")
        logger.info(f"ðŸŽ¯ Model: {self.model_key} ({self.model_config['performance']})")
        logger.info(f"ðŸ“ Queries: {text_queries}")

        start_time = time.time()

        try:
            if self.is_grounding_dino:
                # Grounding DINO expects single text string with proper formatting
                # Use individual terms without combining to avoid compound labels
                text_input = " . ".join(text_queries) + " ."
                inputs = self.processor(images=image, text=text_input, return_tensors="pt")
            else:
                # OWL-ViT expects list of texts
                inputs = self.processor(text=text_queries, images=image, return_tensors="pt")

            if self.device != "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)

            # Post-process outputs (model-specific)
            if self.is_grounding_dino:
                results = self._process_grounding_dino_outputs(
                    outputs, image, text_queries, confidence_threshold
                )
            else:
                results = self._process_owlvit_outputs(
                    outputs, image, text_queries, confidence_threshold
                )

            processing_time = time.time() - start_time

            # Limit detections
            if len(results) > max_detections:
                results = sorted(results, key=lambda x: x['score'], reverse=True)[:max_detections]

            return {
                "image_path": image_path,
                "image_size": image.size,
                "text_queries": text_queries,
                "detections": results,
                "processing_time": processing_time,
                "total_detections": len(results),
                "confidence_threshold": confidence_threshold,
                "model_info": {
                    "model_key": self.model_key,
                    "model_name": self.model_name,
                    "description": self.model_config["description"],
                    "performance": self.model_config["performance"]
                }
            }

        except Exception as e:
            logger.error(f"âŒ Error during object detection: {e}")
            raise

    def _process_grounding_dino_outputs(
        self, outputs, image, text_queries, confidence_threshold
    ) -> List[Dict]:
        """Process Grounding DINO model outputs."""
        target_sizes = torch.tensor([image.size[::-1]])

        results = self.processor.post_process_grounded_object_detection(
            outputs,
            target_sizes=target_sizes,
            threshold=confidence_threshold,
            text_threshold=0.25
        )

        detections = []
        if results[0]["boxes"].numel() > 0:
            boxes = results[0]["boxes"].cpu().numpy()
            scores = results[0]["scores"].cpu().numpy()
            labels = results[0]["labels"]

            for box, score, label in zip(boxes, scores, labels):
                # Clean and map label back to original queries
                cleaned_label = self._clean_grounding_dino_label(label.strip(), text_queries)

                detection = {
                    "box": box.tolist(),  # [x1, y1, x2, y2]
                    "score": float(score),
                    "label": cleaned_label,
                    "label_id": 0  # Grounding DINO doesn't use label IDs the same way
                }
                detections.append(detection)

        return detections

    def _clean_grounding_dino_label(self, raw_label: str, original_queries: List[str]) -> str:
        """
        Clean Grounding DINO labels and map back to original queries.

        Grounding DINO sometimes generates compound labels like "weapon gun"
        when the input contains multiple related terms. This function tries to
        map them back to the most appropriate original query.
        """
        raw_label = raw_label.lower().strip()

        # Direct match first
        for query in original_queries:
            if query.lower() == raw_label:
                return query

        # Partial match - find the best matching original query
        best_match = None
        max_overlap = 0

        for query in original_queries:
            query_lower = query.lower()

            # Check if query is contained in the raw label
            if query_lower in raw_label:
                overlap = len(query_lower)
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_match = query

            # Check if raw label is contained in query
            elif raw_label in query_lower:
                overlap = len(raw_label)
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_match = query

        # If we found a good match, use it
        if best_match and max_overlap > 2:  # Minimum 3 characters overlap
            return best_match

        # Handle common compound cases
        compound_mappings = {
            "weapon gun": "weapon",
            "weapon knife": "weapon",
            "person man": "person",
            "person woman": "person",
            "car vehicle": "car",
            "vehicle car": "vehicle"
        }

        for compound, simple in compound_mappings.items():
            if raw_label == compound and simple in [q.lower() for q in original_queries]:
                return simple

        # As a last resort, return the first word of compound labels
        if " " in raw_label:
            first_word = raw_label.split()[0]
            for query in original_queries:
                if query.lower() == first_word:
                    return query

        # If no mapping found, return the cleaned raw label
        return raw_label

    def _process_owlvit_outputs(
        self, outputs, image, text_queries, confidence_threshold
    ) -> List[Dict]:
        """Process OWL-ViT model outputs."""
        target_sizes = torch.tensor([image.size[::-1]])

        try:
            results = self.processor.post_process_grounded_object_detection(
                outputs=outputs,
                target_sizes=target_sizes,
                threshold=confidence_threshold
            )
        except (AttributeError, TypeError):
            # Fallback to older API
            results = self.processor.post_process_object_detection(
                outputs=outputs,
                target_sizes=target_sizes,
                threshold=confidence_threshold
            )

        detections = []
        if len(results) > 0 and "boxes" in results[0]:
            boxes = results[0]["boxes"].cpu().numpy()
            scores = results[0]["scores"].cpu().numpy()
            labels = results[0]["labels"].cpu().numpy()

            for box, score, label in zip(boxes, scores, labels):
                detection = {
                    "box": box.tolist(),  # [x1, y1, x2, y2]
                    "score": float(score),
                    "label": text_queries[label],
                    "label_id": int(label)
                }
                detections.append(detection)

        return detections

    def visualize_detections(
        self,
        results: Dict,
        output_path: Optional[str] = None,
        show_image: bool = True,
        font_size: int = 12
    ) -> None:
        """
        Visualize detection results with bounding boxes and labels.
        """
        image_path = results["image_path"]
        detections = results["detections"]
        model_info = results["model_info"]

        # Load image
        image = Image.open(image_path)

        # Create figure
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        ax.imshow(image)

        # Colors for different labels
        colors = plt.cm.Set3(torch.linspace(0, 1, max(len(detections), 1)))

        # Draw bounding boxes
        for i, detection in enumerate(detections):
            box = detection["box"]
            score = detection["score"]
            label = detection["label"]

            # Box coordinates (x1, y1, x2, y2)
            x1, y1, x2, y2 = box
            width = x2 - x1
            height = y2 - y1

            # Create rectangle
            rect = patches.Rectangle(
                (x1, y1), width, height,
                linewidth=2,
                edgecolor=colors[i % len(colors)],
                facecolor='none'
            )
            ax.add_patch(rect)

            # Add label with confidence score
            ax.text(
                x1, y1 - 5,
                f"{label}: {score:.3f}",
                fontsize=font_size,
                color=colors[i % len(colors)],
                weight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8)
            )

        ax.set_title(
            f"{model_info['description']}\n"
            f"Found {len(detections)} objects in {results['processing_time']:.2f}s\n"
            f"Performance: {model_info['performance']}",
            fontsize=14,
            weight='bold'
        )
        ax.axis('off')

        plt.tight_layout()

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            logger.info(f"ðŸ’¾ Visualization saved to {output_path}")

        if show_image:
            plt.show()
        else:
            plt.close()

    @staticmethod
    def list_available_models():
        """List all available models with their capabilities."""
        print("ðŸš€ Available Open Vocabulary Detection Models (2024)")
        print("=" * 60)

        for key, config in MODEL_CONFIGS.items():
            status = "âœ… RECOMMENDED" if config["recommended"] else "âš ï¸  BASELINE"
            print(f"\n{status} {key}")
            print(f"   ðŸ“ {config['description']}")
            print(f"   ðŸ“Š Performance: {config['performance']}")
            print(f"   âš¡ Speed: {config['speed']} | ðŸ’¾ Memory: {config['memory']}")

    @staticmethod
    def benchmark_models(image_path: str, queries: List[str]):
        """Benchmark different models on the same image."""
        print("ðŸ Benchmarking Open Vocabulary Detection Models")
        print("=" * 60)

        results = {}

        for model_key in MODEL_CONFIGS.keys():
            print(f"\nðŸ”„ Testing {model_key}...")

            try:
                detector = AdvancedDetector(model_key=model_key)
                result = detector.detect_objects(
                    image_path=image_path,
                    text_queries=queries,
                    confidence_threshold=0.3
                )

                results[model_key] = {
                    "detections": result["total_detections"],
                    "time": result["processing_time"],
                    "performance": MODEL_CONFIGS[model_key]["performance"]
                }

                print(f"   âœ… {result['total_detections']} detections in {result['processing_time']:.2f}s")

            except Exception as e:
                print(f"   âŒ Failed: {e}")
                results[model_key] = {"error": str(e)}

        # Summary
        print(f"\nðŸ“Š Benchmark Summary:")
        for model_key, result in results.items():
            if "error" not in result:
                print(f"   {model_key}: {result['detections']} objects, {result['time']:.2f}s")


def main():
    """Main demonstration function."""
    import sys

    if len(sys.argv) == 1:
        # Demo mode - list models and show usage
        AdvancedDetector.list_available_models()

        print(f"\nðŸ’¡ Usage Examples:")
        print(f"   python advanced_detection.py image.jpg 'person,car,building'")
        print(f"   python advanced_detection.py --model grounding-dino-base image.jpg 'weapon,evidence'")
        print(f"   python advanced_detection.py --benchmark image.jpg 'person,car'")

    elif "--benchmark" in sys.argv:
        # Benchmark mode
        image_path = sys.argv[sys.argv.index("--benchmark") + 1]
        queries = sys.argv[sys.argv.index("--benchmark") + 2].split(',')
        AdvancedDetector.benchmark_models(image_path, queries)

    else:
        # Detection mode
        model_key = "grounding-dino-tiny"  # default

        if "--model" in sys.argv:
            model_idx = sys.argv.index("--model")
            model_key = sys.argv[model_idx + 1]
            image_path = sys.argv[model_idx + 2]
            queries = sys.argv[model_idx + 3].split(',')
        else:
            image_path = sys.argv[1]
            queries = sys.argv[2].split(',') if len(sys.argv) > 2 else ["person", "object"]

        # Run detection
        detector = AdvancedDetector(model_key=model_key)
        results = detector.detect_objects(
            image_path=image_path,
            text_queries=queries,
            confidence_threshold=0.25
        )

        print(f"\nðŸŽ¯ Detection Results:")
        print(f"ðŸ“Š Found {results['total_detections']} objects")
        print(f"â±ï¸  Processing time: {results['processing_time']:.2f}s")

        if results['detections']:
            print(f"\nðŸ” Detected objects:")
            sorted_detections = sorted(results['detections'], key=lambda x: x['score'], reverse=True)
            for i, detection in enumerate(sorted_detections[:10]):
                confidence_emoji = "ðŸŸ¢" if detection['score'] > 0.5 else "ðŸŸ¡" if detection['score'] > 0.3 else "ðŸ”´"
                print(f"   {i+1:2d}. {confidence_emoji} {detection['label']:15s}: {detection['score']:.3f}")

        # Create visualization
        output_path = f"advanced_detection_{model_key}.jpg"
        detector.visualize_detections(
            results,
            output_path=output_path,
            show_image=False
        )
        print(f"\nðŸ’¾ Visualization saved: {output_path}")


if __name__ == "__main__":
    main()